---
phase: 03-performance-reliability
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/retry.rs
  - src/shutdown.rs
  - src/lib.rs
  - src/error.rs
autonomous: true
user_setup: []

must_haves:
  truths:
    - "RetryConfig can be created from Config with max_attempts, base_delay_ms, max_delay_ms"
    - "retry_with_backoff automatically retries transient errors with exponential backoff"
    - "timeout_wrapper enforces overall operation timeout and cancels on budget exhaustion"
    - "GracefulShutdown can handle SIGINT/SIGTERM on Unix and Ctrl+C on Windows"
  artifacts:
    - path: "src/retry.rs"
      provides: "Retry logic with exponential backoff"
      exports: ["RetryConfig", "retry_with_backoff", "timeout_wrapper"]
      contains: "use backoff::ExponentialBackoff"
    - path: "src/shutdown.rs"
      provides: "Signal handling infrastructure"
      exports: ["GracefulShutdown"]
      contains: "tokio::signal"
    - path: "src/lib.rs"
      provides: "Module exports"
      contains: "pub mod retry, pub mod shutdown"
  key_links:
    - from: "src/retry.rs"
      to: "src/cli/commands.rs"
      via: "retry_with_backoff and timeout_wrapper"
      pattern: "retry_with_backoff.*client\.call_tool"
    - from: "src/shutdown.rs"
      to: "src/main.rs"
      via: "GracefulShutdown integration"
      pattern: "run_with_graceful_shutdown"
    - from: "src/retry.rs"
      to: "src/config/mod.rs"
      via: "RetryConfig::from_config"
      pattern: "RetryConfig::from_config"
---

<objective>
Create retry logic with exponential backoff and signal handling infrastructure for graceful shutdown.

Purpose: Implement EXEC-05 (retry with exponential backoff), EXEC-06 (operation timeout), EXEC-07 (retry limits), and CLI-04 (signal handling). Uses backoff crate for production-ready retry patterns and tokio::signal for cross-platform signal handling.

Output: src/retry.rs with RetryConfig, retry_with_backoff, timeout_wrapper; src/shutdown.rs with GracefulShutdown; new error types in error.rs; module exports in lib.rs.
</object>

<execution_context>
@.opencode/get-shit-done/workflows/execute-plan.md
@.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/03-performance-reliability/03-RESEARCH.md
@src/config/mod.rs
@src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Add retry-related error types</name>
  <files>src/error.rs</files>
  <action>Add two new error variants to the McpError enum in src/error.rs:

Add these after the existing error variants (before impl blocks):

```rust
// Retry and timeout errors (EXEC-05, EXEC-06, EXEC-07)
#[error("Operation cancelled (timeout: {}s)", timeout)]
OperationCancelled { timeout: u64 },

#[error("Max retry attempts ({}) exceeded", attempts)]
MaxRetriesExceeded { attempts: u32 },
```

Also add helper methods in the impl McpError block (before From implementations):

```rust
pub fn operation_cancelled(timeout: u64) -> Self {
    Self::OperationCancelled { timeout }
}

pub fn max_retries_exceeded(attempts: u32) -> Self {
    Self::MaxRetriesExceeded { attempts }
}
```

These error types distinguish between timeout cancellation and retry exhaustion, providing clear feedback for different failure modes.</action>
  <verify>Run `cargo check` to verify new error variants compile. Check that exit_code() function handles these errors appropriately (client error code 1 for MaxRetriesExceeded, network error code 3 for OperationCancelled).</verify>
  <done>McpError has OperationCancelled and MaxRetriesExceeded variants with helper methods, code compiles.</done>
</task>

<task type="auto">
  <name>Create retry logic module with backoff</name>
  <files>src/retry.rs</files>
  <action>Create a new file src/retry.rs with retry logic using the backoff crate:

```rust
//! Retry logic with exponential backoff for transient errors.
//!
//! Provides automatic retry with configurable limits and exponential backoff.
//! Implements EXEC-05, EXEC-06, EXEC-07.

use backoff::{ExponentialBackoff, future::retry, Error as BackoffError};
use std::time::Duration;
use tokio::time::timeout;
use crate::error::{McpError, Result};

/// Configuration for retry behavior.
#[derive(Debug, Clone)]
pub struct RetryConfig {
    /// Maximum number of retry attempts (EXEC-07).
    pub max_attempts: u32,

    /// Base delay in milliseconds for exponential backoff (EXEC-07).
    pub base_delay_ms: u64,

    /// Maximum delay in milliseconds between retries.
    pub max_delay_ms: u64,
}

impl RetryConfig {
    /// Create RetryConfig from Config struct.
    pub fn from_config(config: &crate::config::Config) -> Self {
        Self {
            max_attempts: config.retry_max,
            base_delay_ms: config.retry_delay_ms,
            max_delay_ms: 30_000, // 30 seconds cap (research recommendation)
        }
    }

    /// Get the backoff configuration.
    fn backoff(&self) -> ExponentialBackoff {
        ExponentialBackoff {
            current_interval: Duration::from_millis(self.base_delay_ms),
            randomization_factor: 0.5,
            multiplier: 2.0,
            max_interval: Duration::from_millis(self.max_delay_ms),
            max_elapsed_time: None,
            start_time: std::time::Instant::now(),
            clock: backoff::SystemClock,
        }
    }
}

impl Default for RetryConfig {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay_ms: 1000,
            max_delay_ms: 30_000,
        }
    }
}

/// Check if an error is transient (should be retried).
///
/// Transient errors: Timeout, ConnectionError, IOError (EXEC-05).
/// Permanent errors: InvalidJson, InvalidProtocol, ToolNotFound, ServerNotFound.
fn is_transient_error(error: &McpError) -> bool {
    matches!(
        error,
        McpError::Timeout { .. }
            | McpError::ConnectionError { .. }
            | McpError::IOError { .. }
            | McpError::IpcError { .. }
    )
}

/// Execute operation with automatic retry on transient errors.
///
/// Implements EXEC-05: exponential backoff for transient errors.
/// Implements EXEC-07: configurable retry limits.
///
/// # Arguments
/// * `operation` - Async operation to retry
/// * `config` - Retry configuration
///
/// # Returns
/// Result<T> with success value or error after exhausting retries
///
/// # Errors
/// Returns permanent errors immediately, transient errors after max_attempts
pub async fn retry_with_backoff<F, T, Fut>(operation: F, config: &RetryConfig) -> Result<T>
where
    F: Fn() -> Fut + Send + Sync,
    Fut: std::future::Future<Output = Result<T>> + Send,
{
    let mut attempt = 0;
    let backoff = config.backoff();

    retry(backoff, || async {
        attempt += 1;

        match operation().await {
            Ok(value) => Ok(value),
            Err(error) if is_transient_error(&error) => {
                // If this was the last attempt, convert to permanent error
                if attempt >= config.max_attempts {
                    Err(BackoffError::Permanent(McpError::max_retries_exceeded(
                        config.max_attempts,
                    )))
                } else {
                    // Transient error - trigger backoff and retry
                    Err(BackoffError::transient(error))
                }
            }
            Err(error) => {
                // Permanent error - don't retry
                Err(BackoffError::Permanent(error))
            }
        }
    })
    .await
    .map_err(|e| match e {
        BackoffError::Permanent(e) | BackoffError::Transient(e) => e,
        _ => McpError::io_error(std::io::Error::new(
            std::io::ErrorKind::Other,
            "Retry failed",
        )),
    })
}

/// Execute operation with overall timeout.
///
/// Implements EXEC-06: timeout enforcement cancels retries when budget exhausted.
///
/// # Arguments
/// * `operation` - Async operation to execute
/// * `timeout_secs` - Maximum allowed duration
///
/// # Returns
/// Result<T> with success value or timeout error
///
/// # Errors
/// Returns McpError::OperationCancelled if timeout expires
pub async fn timeout_wrapper<F, T, Fut>(operation: F, timeout_secs: u64) -> Result<T>
where
    F: Fn() -> Fut,
    Fut: std::future::Future<Output = Result<T>>,
{
    let duration = Duration::from_secs(timeout_secs);

    match timeout(duration, operation()).await {
        Ok(result) => result,
        Err(_) => {
            tracing::error!("Operation timed out after {}s", timeout_secs);
            Err(McpError::operation_cancelled(timeout_secs))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_retry_config_default() {
        let config = RetryConfig::default();
        assert_eq!(config.max_attempts, 3);
        assert_eq!(config.base_delay_ms, 1000);
        assert_eq!(config.max_delay_ms, 30_000);
    }

    #[test]
    fn test_is_transient_error() {
        assert!(is_transient_error(&McpError::Timeout { timeout: 1 }));
        assert!(is_transient_error(&McpError::IOError {
            source: std::io::Error::new(std::io::ErrorKind::TimedOut, "test"),
        }));
        assert!(!is_transient_error(&McpError::InvalidJson {
            source: serde_json::Error::syntax(
                serde_json::error::ErrorCode::ExpectedColon,
                0,
                0,
            ),
        }));
    }
}
```

Key implementation details:
- Uses backoff crate with ExponentialBackoff per research (includes jitter, cancel-safety)
- Distinguishes transient vs permanent errors (only retry timeouts, connection errors, IO errors)
- Converts to permanent error after max_attempts to prevent infinite loops
- timeout_wrapper wraps any async operation with tokio::time::timeout</action>
  <verify>Run `cargo check` to verify the retry module compiles. Ensure backoff crate features are correct (tokio feature enabled).</verify>
  <done>src/retry.rs exists with RetryConfig, retry_with_backoff, timeout_wrapper, code compiles, tests pass.</done>
</task>

<task type="auto">
  <name>Create signal handling module</name>
  <files>src/shutdown.rs</files>
  <action>Create a new file src/shutdown.rs with signal handling infrastructure:

```rust
//! Graceful signal handling and shutdown utilities.
//!
//! Provides cross-platform signal handling for SIGINT/SIGTERM (Unix)
//! and Ctrl+C (Windows). Implements CLI-04.

use tokio::sync::broadcast;
use tokio::signal;

/// Graceful shutdown handler for signals.
///
/// Allows async operations to respond to termination requests
/// and clean up resources properly.
pub struct GracefulShutdown {
    /// Shutdown sender.
    shutdown_tx: broadcast::Sender<bool>,

    /// Shutdown receiver.
    shutdown_rx: broadcast::Receiver<bool>,
}

impl GracefulShutdown {
    /// Create a new GracefulShutdown handler.
    pub fn new() -> Self {
        let (shutdown_tx, shutdown_rx) = broadcast::channel(1);

        Self {
            shutdown_tx,
            shutdown_rx,
        }
    }

    /// Spawn the signal listener task.
    ///
    /// Listens for SIGINT/SIGTERM on Unix and Ctrl+C on Windows.
    /// Sends shutdown signal when received.
    pub fn spawn_signal_listener(&self) {
        let mut shutdown_tx = self.shutdown_tx.clone();

        tokio::spawn(async move {
            #[cfg(unix)]
            {
                use tokio::signal::unix::{self, SignalKind};

                // Setup signal handlers for POSIX systems
                let mut sigint = unix::signal(SignalKind::interrupt())
                    .expect("Failed to setup SIGINT handler");
                let mut sigterm = unix::signal(SignalKind::terminate())
                    .expect("Failed to setup SIGTERM handler");

                tokio::select! {
                    _ = sigint.recv() => {
                        println!("\nReceived SIGINT (Ctrl+C), shutting down...");
                    }
                    _ = sigterm.recv() => {
                        println!("\nReceived SIGTERM, shutting down...");
                    }
                }
            }

            #[cfg(windows)]
            {
                // Setup Ctrl+C handler for Windows
                if signal::ctrl_c().await.is_ok() {
                    println!("\nReceived shutdown signal, shutting down...");
                }
            }

            // Send shutdown signal to all listeners
            let _ = shutdown_tx.send(true);
        });
    }

    /// Subscribe to shutdown notifications.
    ///
    /// Operations holding a receiver can check for shutdown requests.
    pub fn subscribe(&self) -> broadcast::Receiver<bool> {
        self.shutdown_tx.subscribe()
    }

    /// Check if shutdown was requested.
    ///
    /// Returns true if shutdown signal was sent.
    pub fn is_shutdown_requested(&mut self) -> bool {
        if let Ok(Ok(_)) = self.shutdown_rx.try_recv() {
            true
        } else {
            false
        }
    }
}

impl Default for GracefulShutdown {
    fn default() -> Self {
        Self::new()
    }
}

/// Run an async operation with graceful shutdown support.
///
/// Automatically spawns signal listener and cancels operation on shutdown.
///
/// # Arguments
/// * `op` - Async operation to run
///
/// # Returns
/// Result<T> from the operation or Ok(()) if shutdown occurred
///
/// # Example
/// ```rust,ignore
/// let shutdown = GracefulShutdown::new();
/// shutdown.spawn_signal_listener();
///
/// let result = run_with_graceful_shutdown(
///     || async {
///         // Your async operation here
///         Ok::<_, McpError>("success")
///     },
///     shutdown.subscribe(),
/// ).await?;
/// ```
pub async fn run_with_graceful_shutdown<F, T, Fut>(op: F, mut shutdown_rx: broadcast::Receiver<bool>) -> Result<T>
where
    F: Fn() -> Fut,
    Fut: std::future::Future<Output = crate::error::Result<T>>,
{
    tokio::select! {
        result = op() => result,
        _ = shutdown_rx.recv() => {
            println!("Shutting down gracefully...");
            // Return success on shutdown (user initiated)
            Err(crate::error::McpError::io_error(
                std::io::Error::new(std::io::ErrorKind::Interrupted, "Shutdown requested")
            ))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_graceful_shutdown_default() {
        let shutdown = GracefulShutdown::default();
        // Create a receiver to ensure the system is initialized
        let _rx = shutdown.subscribe();
    }

    #[test]
    fn test_is_shutdown_requested() {
        let mut shutdown = GracefulShutdown::new();
        assert!(!shutdown.is_shutdown_requested());

        // Send shutdown signal
        let _ = shutdown.shutdown_tx.send(true);
        assert!(shutdown.is_shutdown_requested());
    }
}
```

Key implementation details:
- Cross-platform: Unix uses SignalKind::interrupt/terminate, Windows uses signal::ctrl_c()
- Uses tokio::sync::broadcast for channel pattern (multiple subscribers, one sender)
- spawn_signal_listener is async-friendly (tokio::spawn)
- run_with_graceful_shutdown provides wrapper pattern for easy integration</action>
  <verify>Run `cargo check` to verify the shutdown module compiles. Ensure tokio signal features are available (tokio "full" features include signal).</verify>
  <done>src/shutdown.rs exists with GracefulShutdown and run_with_graceful_shutdown, code compiles, tests pass.</done>
</task>

<task type="auto">
  <name>Add retry and shutdown modules to lib exports</name>
  <files>src/lib.rs</files>
  <action>Add the retry and shutdown modules to src/lib.rs exports:

Find the module declaration section in lib.rs and add (after existing modules):
```rust
pub mod retry;
pub mod shutdown;
```

If modules are declared elsewhere, add them in a consistent pattern matching the existing code.

This makes RetryConfig, retry_with_backoff, timeout_wrapper, and GracefulShutdown available to CLI commands and main.rs.</action>
  <verify>Run `cargo check` to verify both modules are accessible. Test with `cargo test` to ensure all unit tests pass.</verify>
  <done>retry and shutdown modules exported in lib.rs, code compiles, utilities accessible from other modules.</done>
</task>

</tasks>

<verification>
Run `cargo check` to verify all changes compile together:
- src/retry.rs with RetryConfig, retry_with_backoff, timeout_wrapper
- src/shutdown.rs with GracefulShutdown
- src/error.rs with OperationCancelled and MaxRetriesExceeded
- lib.rs exports retry and shutdown modules

Expected: No compilation errors, retry and shutdown utilities ready for CLI integration.

Run `cargo test` to verify unit tests in retry and shutdown modules pass.
</verification>

<success_criteria>
1. src/retry.rs module exists with RetryConfig (from_config, default), retry_with_backoff, timeout_wrapper
2. retry_with_backoff distinguishes transient vs permanent errors, uses backoff::ExponentialBackoff
3. timeout_wrapper uses tokio::time::timeout, returns OperationCancelled on timeout
4. src/shutdown.rs module exists with GracefulShutdown (spawn_signal_listener, subscribe) and run_with_graceful_shutdown
5. Signal handling cross-platform (SIGINT/SIGTERM on Unix, Ctrl+C on Windows)
6. New error types (OperationCancelled, MaxRetriesExceeded) in McpError with helper methods
7. Modules exported in lib.rs, code compiles, tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-performance-reliability/03-03-SUMMARY.md`
</output>
