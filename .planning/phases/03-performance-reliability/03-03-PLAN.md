---
phase: 03-performance-reliability
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/retry.rs
  - src/error.rs
  - src/lib.rs
autonomous: true
user_setup: []

must_haves:
  truths:
    - "RetryConfig can be created from Config with max_attempts, base_delay_ms, max_delay_ms"
    - "retry_with_backoff automatically retries transient errors with exponential backoff"
    - "timeout_wrapper enforces overall operation timeout and cancels on budget exhaustion"
  artifacts:
    - path: "src/retry.rs"
      provides: "Retry logic with exponential backoff"
      exports: ["RetryConfig", "retry_with_backoff", "timeout_wrapper"]
      contains: "use backoff::ExponentialBackoff"
    - path: "src/error.rs"
      provides: "Retry error types"
      exports: ["McpError::OperationCancelled", "McpError::MaxRetriesExceeded"]
      contains: "OperationCancelled|MaxRetriesExceeded"
    - path: "src/lib.rs"
      provides: "Retry module exports"
      contains: "pub mod retry"
  key_links:
    - from: "src/retry.rs"
      to: "src/cli/commands.rs"
      via: "retry_with_backoff and timeout_wrapper"
      pattern: "retry_with_backoff.*client\.call_tool"
    - from: "src/retry.rs"
      to: "src/config/mod.rs"
      via: "RetryConfig::from_config"
      pattern: "RetryConfig::from_config"
---

<objective>
Create retry logic with exponential backoff and timeout enforcement.

Purpose: Implement EXEC-05 (retry with exponential backoff), EXEC-06 (operation timeout), EXEC-07 (retry limits). Uses backoff crate for production-ready retry patterns with jitter and cancel-safety.

Output: src/retry.rs with RetryConfig, retry_with_backoff, timeout_wrapper; retry-related error types in error.rs; retry module export in lib.rs.
</objective>

<execution_context>
@.opencode/get-shit-done/workflows/execute-plan.md
@.opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/03-performance-reliability/03-RESEARCH.md
@src/config/mod.rs
@src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Add retry-related error types</name>
  <files>src/error.rs</files>
  <action>Add two new error variants to the McpError enum in src/error.rs:

Add these after the existing error variants (before impl blocks):

```rust
// Retry and timeout errors (EXEC-05, EXEC-06, EXEC-07)
#[error("Operation cancelled (timeout: {}s)", timeout)]
OperationCancelled { timeout: u64 },

#[error("Max retry attempts ({}) exceeded", attempts)]
MaxRetriesExceeded { attempts: u32 },
```

Also add helper methods in the impl McpError block (before From implementations):

```rust
pub fn operation_cancelled(timeout: u64) -> Self {
    Self::OperationCancelled { timeout }
}

pub fn max_retries_exceeded(attempts: u32) -> Self {
    Self::MaxRetriesExceeded { attempts }
}
```

These error types distinguish between timeout cancellation and retry exhaustion, providing clear feedback for different failure modes.</action>
  <verify>Run `cargo check` to verify new error variants compile. Check that exit_code() function handles these errors appropriately (client error code 1 for MaxRetriesExceeded, network error code 3 for OperationCancelled).</verify>
  <done>McpError has OperationCancelled and MaxRetriesExceeded variants with helper methods, code compiles.</done>
</task>

<task type="auto">
  <name>Create retry logic module with backoff</name>
  <files>src/retry.rs</files>
  <action>Create a new file src/retry.rs with retry logic using the backoff crate:

```rust
//! Retry logic with exponential backoff for transient errors.
//!
//! Provides automatic retry with configurable limits and exponential backoff.
//! Implements EXEC-05, EXEC-06, EXEC-07.

use backoff::{ExponentialBackoff, future::retry, Error as BackoffError};
use std::time::Duration;
use tokio::time::timeout;
use crate::error::{McpError, Result};

/// Configuration for retry behavior.
#[derive(Debug, Clone)]
pub struct RetryConfig {
    /// Maximum number of retry attempts (EXEC-07).
    pub max_attempts: u32,

    /// Base delay in milliseconds for exponential backoff (EXEC-07).
    pub base_delay_ms: u64,

    /// Maximum delay in milliseconds between retries.
    pub max_delay_ms: u64,
}

impl RetryConfig {
    /// Create RetryConfig from Config struct.
    pub fn from_config(config: &crate::config::Config) -> Self {
        Self {
            max_attempts: config.retry_max,
            base_delay_ms: config.retry_delay_ms,
            max_delay_ms: 30_000, // 30 seconds cap (research recommendation)
        }
    }

    /// Get the backoff configuration.
    fn backoff(&self) -> ExponentialBackoff {
        ExponentialBackoff {
            current_interval: Duration::from_millis(self.base_delay_ms),
            randomization_factor: 0.5,
            multiplier: 2.0,
            max_interval: Duration::from_millis(self.max_delay_ms),
            max_elapsed_time: None,
            start_time: std::time::Instant::now(),
            clock: backoff::SystemClock,
        }
    }
}

impl Default for RetryConfig {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay_ms: 1000,
            max_delay_ms: 30_000,
        }
    }
}

/// Check if an error is transient (should be retried).
///
/// Transient errors: Timeout, ConnectionError, IOError (EXEC-05).
/// Permanent errors: InvalidJson, InvalidProtocol, ToolNotFound, ServerNotFound.
fn is_transient_error(error: &McpError) -> bool {
    matches!(
        error,
        McpError::Timeout { .. }
            | McpError::ConnectionError { .. }
            | McpError::IOError { .. }
            | McpError::IpcError { .. }
    )
}

/// Execute operation with automatic retry on transient errors.
///
/// Implements EXEC-05: exponential backoff for transient errors.
/// Implements EXEC-07: configurable retry limits.
///
/// # Arguments
/// * `operation` - Async operation to retry
/// * `config` - Retry configuration
///
/// # Returns
/// Result<T> with success value or error after exhausting retries
///
/// # Errors
/// Returns permanent errors immediately, transient errors after max_attempts
pub async fn retry_with_backoff<F, T, Fut>(operation: F, config: &RetryConfig) -> Result<T>
where
    F: Fn() -> Fut + Send + Sync,
    Fut: std::future::Future<Output = Result<T>> + Send,
{
    let mut attempt = 0;
    let backoff = config.backoff();

    retry(backoff, || async {
        attempt += 1;

        match operation().await {
            Ok(value) => Ok(value),
            Err(error) if is_transient_error(&error) => {
                // If this was the last attempt, convert to permanent error
                if attempt >= config.max_attempts {
                    Err(BackoffError::Permanent(McpError::max_retries_exceeded(
                        config.max_attempts,
                    )))
                } else {
                    // Transient error - trigger backoff and retry
                    Err(BackoffError::transient(error))
                }
            }
            Err(error) => {
                // Permanent error - don't retry
                Err(BackoffError::Permanent(error))
            }
        }
    })
    .await
    .map_err(|e| match e {
        BackoffError::Permanent(e) | BackoffError::Transient(e) => e,
        _ => McpError::io_error(std::io::Error::new(
            std::io::ErrorKind::Other,
            "Retry failed",
        )),
    })
}

/// Execute operation with overall timeout.
///
/// Implements EXEC-06: timeout enforcement cancels retries when budget exhausted.
///
/// # Arguments
/// * `operation` - Async operation to execute
/// * `timeout_secs` - Maximum allowed duration
///
/// # Returns
/// Result<T> with success value or timeout error
///
/// # Errors
/// Returns McpError::OperationCancelled if timeout expires
pub async fn timeout_wrapper<F, T, Fut>(operation: F, timeout_secs: u64) -> Result<T>
where
    F: Fn() -> Fut,
    Fut: std::future::Future<Output = Result<T>>,
{
    let duration = Duration::from_secs(timeout_secs);

    match timeout(duration, operation()).await {
        Ok(result) => result,
        Err(_) => {
            tracing::error!("Operation timed out after {}s", timeout_secs);
            Err(McpError::operation_cancelled(timeout_secs))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_retry_config_default() {
        let config = RetryConfig::default();
        assert_eq!(config.max_attempts, 3);
        assert_eq!(config.base_delay_ms, 1000);
        assert_eq!(config.max_delay_ms, 30_000);
    }

    #[test]
    fn test_is_transient_error() {
        assert!(is_transient_error(&McpError::Timeout { timeout: 1 }));
        assert!(is_transient_error(&McpError::IOError {
            source: std::io::Error::new(std::io::ErrorKind::TimedOut, "test"),
        }));
        assert!(!is_transient_error(&McpError::InvalidJson {
            source: serde_json::Error::syntax(
                serde_json::error::ErrorCode::ExpectedColon,
                0,
                0,
            ),
        }));
    }
}
```

Key implementation details:
- Uses backoff crate with ExponentialBackoff per research (includes jitter, cancel-safety)
- Distinguishes transient vs permanent errors (only retry timeouts, connection errors, IO errors)
- Converts to permanent error after max_attempts to prevent infinite loops
- timeout_wrapper wraps any async operation with tokio::time::timeout</action>
  <verify>Run `cargo check` to verify the retry module compiles. Ensure backoff crate features are correct (tokio feature enabled).</verify>
  <done>src/retry.rs exists with RetryConfig, retry_with_backoff, timeout_wrapper, code compiles, tests pass.</done>
</task>

<task type="auto">
  <name>Add retry module to lib exports</name>
  <files>src/lib.rs</files>
  <action>Add the retry module to src/lib.rs exports:

Find the module declaration section in lib.rs and add (after existing modules):
```rust
pub mod retry;
```

If modules are declared elsewhere, add them in a consistent pattern matching the existing code.

This makes RetryConfig, retry_with_backoff, and timeout_wrapper available to CLI commands.</action>
  <verify>Run `cargo check` to verify the retry module is accessible. Test with `cargo test` to ensure all unit tests pass.</verify>
  <done>retry module exported in lib.rs, code compiles, utilities accessible from other modules.</done>
</task>

</tasks>

<verification>
Run `cargo check` to verify all changes compile together:
- src/retry.rs with RetryConfig, retry_with_backoff, timeout_wrapper
- src/error.rs with OperationCancelled and MaxRetriesExceeded
- lib.rs exports retry module

Expected: No compilation errors, retry utilities ready for CLI integration.

Run `cargo test` to verify unit tests in retry module pass.
</verification>

<success_criteria>
1. src/retry.rs module exists with RetryConfig (from_config, default), retry_with_backoff, timeout_wrapper
2. retry_with_backoff distinguishes transient vs permanent errors, uses backoff::ExponentialBackoff
3. timeout_wrapper uses tokio::time::timeout, returns OperationCancelled on timeout
4. New error types (OperationCancelled, MaxRetriesExceeded) in McpError with helper methods
5. Retry module exported in lib.rs, code compiles, tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-performance-reliability/03-03-SUMMARY.md`
</output>
